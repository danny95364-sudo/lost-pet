
import pandas as pd
import requests
import io
import os
import urllib3
from db import upsert_clinic, init_db

# é—œé–‰ SSL è­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)


class PetResourcesCrawlerV11:
    def __init__(self):
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        }
        init_db()

    def fetch_data_robust(self, url, name, target_columns_keywords):
        """
        æœ€å¼·éŸŒçš„æŠ“å–å‡½å¼ (V9æ ¸å¿ƒ)ï¼šåŒæ™‚æ”¯æ´ JSON/CSVï¼Œä¸¦å…·å‚™è‡ªå‹•æ¬„ä½æ¸…æ´—åŠŸèƒ½
        """
        print(f"ğŸ“¥ æ­£åœ¨ä¸‹è¼‰ã€{name}ã€‘...")
        try:
            # å˜—è©¦åŠ å…¥ &IsOD=1 åƒæ•¸ï¼Œæœ‰æ™‚å€™èƒ½æŠ“åˆ°æ›´å¤šè³‡æ–™
            if "?" in url:
                url += "&IsOD=1"
            else:
                url += "?IsOD=1"

            response = requests.get(url, headers=self.headers, verify=False, timeout=30)
            
            df = pd.DataFrame()
            is_json = False

            # 1. å˜—è©¦ç•¶ä½œ JSON è§£æ
            try:
                content_start = response.content[:10].decode('utf-8', errors='ignore').strip()
                if content_start.startswith('[') or content_start.startswith('{'):
                    json_data = response.json()
                    df = pd.DataFrame(json_data)
                    is_json = True
            except:
                pass

            # 2. å¦‚æœå¤±æ•—ï¼Œå˜—è©¦ç•¶ä½œ CSV è§£æ
            if not is_json or df.empty:
                content = response.content
                encodings = ['utf-8', 'utf-8-sig', 'big5', 'cp950']
                for enc in encodings:
                    try:
                        df = pd.read_csv(io.StringIO(content.decode(enc)), on_bad_lines='skip')
                        if not df.empty and len(df.columns) > 1:
                            break
                    except:
                        continue

            if df.empty:
                print(f"   âŒ {name} è®€å–å¤±æ•—")
                return pd.DataFrame()

            # æ¸…æ´—
            df.columns = df.columns.str.strip()
            final_col_map = {}
            for target_key, keywords in target_columns_keywords.items():
                for col in df.columns:
                    if any(k in col for k in keywords):
                        final_col_map[col] = target_key
                        break
            
            if not final_col_map:
                print(f"   âš ï¸ {name} æ¬„ä½å°æ‡‰å¤±æ•—")
                return pd.DataFrame()

            df.rename(columns=final_col_map, inplace=True)
            found_cols = [c for c in final_col_map.values() if c in df.columns]
            df = df[found_cols].fillna('')
            
            print(f"   âœ… æˆåŠŸè®€å– {len(df)} ç­†åŸå§‹è³‡æ–™")
            return df

        except Exception as e:
            print(f"   âŒ {name} ç™¼ç”ŸéŒ¯èª¤: {e}")
            return pd.DataFrame()

    def get_vet_clinics(self):
        url = "https://data.moa.gov.tw/Service/OpenData/DataFileService.aspx?UnitId=078"
        keywords = {"name": ["æ©Ÿæ§‹åç¨±"], "tel": ["é›»è©±"], "address": ["åœ°å€"], "doctor_name": ["ç¸é†«", "è² è²¬äºº"]}

        df = self.fetch_data_robust(url, "å‹•ç‰©é†«é™¢", keywords)

        if not df.empty:
            print(f"   ğŸ”¨ æ­£åœ¨ç”Ÿæˆ Google Maps é€£çµ...")
            df['google_map_link'] = df.apply(
                lambda row: f"https://www.google.com/maps/search/?api=1&query={row['name']}", axis=1
            )
        return df

    def save_to_db(self, df, type_name):
        if df.empty:
            return
            
        print(f"   ğŸ’¾ æ­£åœ¨å­˜å…¥è³‡æ–™åº« ({type_name})...")
        count = 0
        for _, row in df.iterrows():
            data = row.to_dict()
            if type_name == 'vet':
                upsert_clinic(data)
                count += 1
        print(f"   âœ… å·²æ›´æ–° {count} ç­† {type_name} è³‡æ–™")


if __name__ == "__main__":
    crawler = PetResourcesCrawlerV11()
    print("=== ğŸš€ å¯µç‰©è³‡æºçˆ¬èŸ²å•Ÿå‹• ===")

    # 1. é†«é™¢
    df_vet = crawler.get_vet_clinics()
    crawler.save_to_db(df_vet, "vet")
    
    print("\n=== ğŸ‰ è³‡æ–™æ›´æ–°å®Œæˆ ===")